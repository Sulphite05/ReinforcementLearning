{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an Environment and an Agent who is the learner and decision maker. \n",
    "- The agent interacts with environment by taking certain actions on it.\n",
    "- Those actions(A) along with some other factors can result in changing the state of the environemnt.\n",
    "- Environment in turn gives the agent an observation(O) and a reward(R).\n",
    "- The sole purpose of the agent is to maximise the total amount of reward returned by the envronment called Return.\n",
    "- Return is calculated at every step showing the total reward gathered at every step after the current step.\n",
    "- It is typically written with gamma called the discount factor.\n",
    "- It hinders the ability of agent to look too far into the future.\n",
    "- The observation is just the state of the environment after that action.\n",
    "- It can be fully or partially-observable.\n",
    "- If the trajectory of A(t), O(t+1), R(t+1) stops at some point then the task is episodic otherwise continuous.\n",
    "- The behavior of agent in environment is the Policy(pi) that takes O as input and returns A as output.\n",
    "- When an agent sees an O, it tries to estimate a return using a function called value.\n",
    "- So there are two functions, policy and value.\n",
    "- One gives all the actions and the other estimates the return for each action.\n",
    "- Policy can either be deterministic or stochastic.\n",
    "- The former gives out a discrete action that has to be taken.\n",
    "- The latter will return the probability for all the actions in the problem.\n",
    "- Rewards indicate what is good for the agent in an immediate sense.\n",
    "- Value function indicates what is good in the long run.\n",
    "- This interaction has a fancy name called the Markov Decision process.\n",
    "- The future is independent of the past given the present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
